# Quick Reference: Artifact Development Workflow

## Before Writing ANY Aggregation Code

```bash
# Step 1: Sync latest archive sample (200 most recent entries)
python scripts/sync_archive_sample.py --n 200

# Step 2: Read the schema
cat data/schema/ARCHIVE_ENTRY_FULL_SCHEMA.txt

# Step 3: Inspect actual data
python -c "import json; f = open('data/samples/cryptobot_latest_tail200.jsonl', 'r'); entry = json.loads(f.readline()); print(json.dumps(entry, indent=2))"
```

## Archive Data Source

**SSOT Archive Location:** `D:\Sentiment-Data\CryptoBot\data\archive\`

The archive contains dated folders (YYYYMMDD format) with compressed `.jsonl.gz` files. The sync script:
- Automatically finds the latest archive folder
- Extracts the most recent N entries (tail, not head)
- Supports both `.jsonl` and `.jsonl.gz` formats
- Outputs to `data/samples/cryptobot_latest_tail200.jsonl`
- Generates metadata in `data/samples/cryptobot_latest_tail200.meta.json`

```bash
# Sync with custom parameters
python scripts/sync_archive_sample.py --n 200 --archive-path "D:\Sentiment-Data\CryptoBot\data\archive"
```

## Field Path Verification Template

```python
import json

field_path = "twitter_sentiment_windows.last_cycle.YOUR_FIELD_HERE"
found = 0
total = 20

with open('data/samples/cryptobot_latest_tail200.jsonl', 'r') as f:
    for i, line in enumerate(f):
        if i >= total:
            break
        entry = json.loads(line)
        
        # Navigate path safely
        value = entry.get('twitter_sentiment_windows', {}).get('last_cycle', {}).get('YOUR_FIELD_HERE')
        if value is not None:
            found += 1
            if i == 0:  # Print example from first entry
                print(f"Example value: {value}")

print(f"\nPath present: {found}/{total} entries ({found/total*100:.0f}%)")
print(f"Decision: {'VERIFIED (>=90%)' if found/total >= 0.9 else 'MISSING (<90%)'}")
```

## Decision Logic

- **≥ 90% availability** → Mark as VERIFIED, use in artifacts
- **< 90% availability** → Mark as MISSING, set `available: false` with exact path and count

## Files to Reference

- `data/field_coverage_report.json` - **SSOT for field availability** (Phase 1A output, 5065 paths indexed)
- `data/canonical_fields.json` - **SSOT for field selection** (Phase 1B output, 170 included, 454 excluded)
- `data/schema/ARCHIVE_ENTRY_FULL_SCHEMA.txt` - Canonical v7 structure
- `docs/PROJECT_GUIDE.md` - Full workflow documentation
- `.github/copilot-instruction.md` - Mandatory investigation rules

## Field Availability: Use Phase 1A Report

**Do NOT hardcode field availability assumptions.**

Instead, always reference `data/field_coverage_report.json` which contains:
- 5065 unique field paths discovered from 147 v7 entries
- Exact presence counts (present/missing) for every path
- Generated by `scripts/inspect_field_coverage.py`

To check if a field exists:
```python
import json
with open('data/field_coverage_report.json', 'r') as f:
    report = json.load(f)

# Navigate to field group and check path
for group, fields in report['field_groups'].items():
    if 'your.field.path' in fields:
        presence = fields['your.field.path']['present']
        total = report['entries_scanned']
        pct = presence / total * 100
        print(f"Field present in {pct:.1f}% of entries")
```

## Remember

❌ **DON'T:**
- Assume field names
- Use "maybe this field is called..."
- Write code before inspection
- Claim features exist when they don't

✅ **DO:**
- Sync sample first
- Verify paths in 20+ entries
- Document missing paths with counts
- Show "Not available yet" with exact reasons

---

## Phase 1B: Canonical Field Selection

**Purpose:** Phase 1B creates an explicit, documented map of which fields will be used in dataset artifacts.

**Key files:**
- `data/canonical_fields.json` - Machine-readable field selection (170 included, 454 excluded)
- `data/canonical_fields.md` - Human-readable documentation and rationale

**Selection criteria:**
- >= 90% availability (or explicit justification for >= 70%)
- Semantic value for dataset-level insights
- Excludes granular entity-level data (mentions, cashtags, tags, author names)

**Usage in artifact builders:**

```python
import json

# Load canonical field map
with open('data/canonical_fields.json', 'r') as f:
    canonical = json.load(f)

# Get included fields for a specific group
sentiment_fields = [f['path'] for f in canonical['groups']['sentiment_last_cycle']['included']]

# Use ONLY these paths - no hardcoding!
for entry in entries:
    for path in sentiment_fields:
        value = get_nested_value(entry, path)  # Safe navigation
        # ... process ...
```

**Why this matters:**
- **Single source of truth** - All artifact builders reference the same field list
- **No assumptions** - Every field verified against real data
- **Explicit exclusions** - Documents why fields were rejected
- **Version control** - Changes to field selection are tracked

**Future artifact builders MUST:**
1. Import `canonical_fields.json` at the start
2. Use ONLY fields in the `included` arrays
3. Never hardcode field paths
4. If a new field is needed, update the canonical map first (with justification)

---

## Phase 1B: Coverage Table for Website

**Purpose:** Generate `public/data/coverage_table.json` for the "What We Collect" section on the Dataset page.

**Build command:**
```bash
python scripts/build_coverage_table.py
```

**Test command:**
```bash
python scripts/test_coverage_table.py
```

**Verification command:**
```bash
python scripts/verify_coverage_table.py
```

**Output location:**
- `public/data/coverage_table.json`

**Requirements:**
- NO 0% rows (excluded groups with 0% presence)
- ALL examples are REAL numeric values computed from sample data
- Examples use median, p10-p90, or counts from `data/samples/cryptobot_latest_head200.jsonl`
- ASCII-only output
- Uses canonical field paths only

**Example computation methods:**
- Numeric fields: median value (e.g., "14.2" for spread_bps)
- Score ranges: p10-p90 (e.g., "-0.04 to 0.88")
- Counts: median count or array length (e.g., "742" snapshots)
- Percentages: computed from sample (e.g., "0.7%" silence rate)
- No descriptive placeholders allowed

**Structure:**
```json
{
  "generated_at_utc": "2026-01-02T10:44:54Z",
  "entries_scanned": 147,
  "rows": [
    {
      "group": "market_microstructure",
      "label": "Market Microstructure",
      "present_pct": 100.0,
      "checks": [
        {
          "path": "derived.spread_bps",
          "label": "Spread (bps)",
          "present_pct": 100.0,
          "example": "14.2",
          "notes": "Median bid-ask spread in basis points"
        }
      ]
    }
  ]
}
```

**Included groups (8):**
1. Market Microstructure (100%)
2. Liquidity Metrics (100%)
3. Order Book Depth (100%)
4. Spot Prices (100%)
5. Sentiment (Last Cycle) (100%)
6. Sentiment (Last 2 Cycles) (100%)
7. Activity vs Silence (100%)
8. Engagement & Authors (73.5%)

**Excluded groups (1):**
- Sampling Density (0% - all fields missing)

---

## Phase 2A: Descriptive Behavior Artifacts

**Purpose:** Generate descriptive statistics about dataset behavior for Updates/Research pages (NO predictive claims).

**Build command:**
```bash
python scripts/build_phase2a_artifacts.py
```

**Test command:**
```bash
python scripts/test_phase2a_artifacts.py
```

**Output files:**
- `public/data/activity_regimes.json` - Tweet volume bins with descriptive stats
- `public/data/sampling_density.json` - Sampling resolution quality metrics
- `public/data/session_lifecycle.json` - Monitoring window lifecycle patterns

**Artifacts:**

1. **activity_regimes.json**
   - Bins entries by tweet volume (0, 1-2, 3-9, 10-24, 25-49, 50+ posts)
   - Shows n_entries, share_pct, median spread/liquidity per bin
   - NO predictive claims - descriptive only

2. **sampling_density.json**
   - Sample count distribution (median, p10, p90, histogram)
   - Spot price sample lengths
   - Histogram buckets: <600, 600-699, 700-749, 750-799, 800-899, 900+

3. **session_lifecycle.json**
   - Monitoring window duration stats (median, p10, p90)
   - Admission hour distribution (0-23)
   - Definition: One watchlist window from added_ts to expires_ts

**Rules:**
- Use ONLY paths from `field_coverage_report.json` (Phase 1A SSOT)
- NO correlations, NO predictive claims
- Deterministic output (except timestamp)
- ASCII-only JSON

**Field discovery:**
- Builder auto-discovers paths from SSOT
- If field unavailable, artifact includes unavailable_reason
- No hardcoded assumptions
---

## Phase 3B: Public Sample Entries

**Purpose:** Generate browsable public preview with 100 full v7 entries for FREE tier ("proof of depth" monetization).

**Build command:**
```bash
python scripts/build_public_sample_entries.py
```

**Test command:**
```bash
python scripts/test_public_sample_entries.py
```

**Output files:**
- `public/data/sample_entries_v7.json` - JSON artifact with metadata wrapper + 100 entries
- `public/data/sample_entries_v7.jsonl` - Downloadable JSONL (one entry per line)

**Features:**
- **Deterministic:** First 100 entries from sample (no randomness)
- **Full entries:** NO field removal (proves data richness)
- **Quantity-limited:** 100 entries insufficient for ML training
- **Dual format:** JSON for UI loader + JSONL for download
- **Disclaimers:** Explicit "Not suitable for training" warning

**Display fields (6):**
1. `symbol` - Ticker
2. `derived.spread_bps` - Spread in basis points
3. `derived.liq_global_pct` - Liquidity percentile
4. `twitter_sentiment_windows.last_2_cycles.posts_total` - Posts count
5. `twitter_sentiment_windows.last_2_cycles.hybrid_decision_stats.mean_score` - Sentiment score
6. `meta.added_ts` - Entry timestamp

**Frontend features:**
- Browsable table (sortable columns)
- Expandable JSON per row
- Search by symbol (real-time filtering)
- Download link to JSONL

**Monetization strategy:**
- FREE: 100 entries prove depth but not ML-usable
- PAID: Full dataset access (future phase)