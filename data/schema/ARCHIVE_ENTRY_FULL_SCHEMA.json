[
  {
    "symbol": "STRING: The cryptocurrency trading pair symbol in BASEQUOTE format (e.g., 'ALTUSDC' where ALT is the base asset and USDC is the quote currency). This is the exchange symbol used on Binance spot market. The symbol identifies which coin is being tracked in this watchlist session. Pattern: 3-15 uppercase alphanumeric characters. Source: Captured at session admission from the universe scanning loop in the main bot.",
    "snapshot_ts": "STRING (ISO 8601 with Z suffix): UTC timestamp of when this watchlist entry was first created/admitted. Format: 'YYYY-MM-DDTHH:MM:SS.sssZ'. This marks the start of the 2-hour tracking session. Source: Generated by datetime.now(timezone.utc) in core/watchlist_builder.py at the moment of entry construction.",
    "meta": {
      "schema_version": "INTEGER: The schema version number for this document structure. Current production version is 7. Version history: v5 (baseline with twitter_sentiment_windows), v6 (added dual-window sentiment), v7 (moved sentiment metadata to twitter_sentiment_meta). Used for schema migration and backward compatibility. Source: Set by build_meta_v7() in core/watchlist_builder.py based on WATCHLIST_SCHEMA_VERSION env var.",
      "source": "STRING: Identifies which data sources are present in this snapshot. Possible values: 'spot' (spot market only), 'futures' (futures only), 'spot+usdsm-futures' (spot + USD-margined futures), 'spot+coinm-futures' (spot + coin-margined futures). Most entries use 'spot+usdsm-futures' as the bot fetches both spot and USDT-margined perpetual futures data. Source: Constant META_SOURCE in core/watchlist_builder.py.",
      "universe_page": "INTEGER (0-indexed): The scan page index within the current universe of tracked coins. The bot scans coins in pages to manage API rate limits. Page 0 contains the first batch of coins. Source: Captured from scorer._current_page at admission time in build_meta_v5().",
      "universe_page_size": "INTEGER: Number of coins per page during universe scanning. Default is 40-50 coins per page. Controls how many coins are fetched and scored in each scanning batch. Source: From TOP_POOL environment variable, defaulting to 50.",
      "normalization_version": "STRING: Version of the normalization algorithm used for derived metrics. Currently 'v1'. Reserved for future normalization changes. Source: Hardcoded in build_meta_v5().",
      "scoring_version": "STRING: Scoring system version. 'v1' = legacy weighted sum scoring. 'v2' = Stage-1/Stage-2 scoring with dynamic gates (current production). v2 uses percentile-based dynamic thresholds that adapt to market conditions. Source: SCORING_VERSION environment variable, defaults to 'v2'.",
      "pairbonus_version": "STRING: PairBonus feature schema version. 'v0' = telemetry scaffolding with zero bonuses (no behavior change). Reserved for future pair-correlation bonuses. Source: PAIRBONUS_VERSION environment variable, defaults to 'v0'.",
      "futures_api_base": "STRING: The Binance API domain used to fetch futures data. 'fapi.binance.com' = USD-margined (USDT-M) futures. 'dapi.binance.com' = coin-margined (COIN-M) futures. Determines which perpetual contract market is queried. Source: Hardcoded in build_meta_v5(), typically 'fapi.binance.com'.",
      "universe_snapshot_id": "STRING: Unique identifier for the universe state snapshot when this entry was created. Used to link entries to a specific point-in-time view of the coin universe. 'BOOTSTRAP' indicates the initial startup state before dynamic universe updates. Source: Generated by _get_universe_linkage() in core/watchlist_builder.py.",
      "universe_snapshot_ts": "STRING (ISO 8601): UTC timestamp of the universe snapshot this entry is linked to. Allows correlation of entries created during the same universe scan cycle. Source: Generated by _get_universe_linkage() in core/watchlist_builder.py.",
      "universe_snapshot_lag_sec": "FLOAT: Time in seconds between the universe snapshot and this entry's creation. Near-zero values indicate the entry was created immediately after the universe scan. Larger values may indicate processing delays. Source: Computed in _get_universe_linkage().",
      "added_ts": "STRING (ISO 8601): UTC timestamp of when this session was admitted to the active watchlist. This is the official session start time. Format: 'YYYY-MM-DDTHH:MM:SS.ssssssZ'. Source: Set by admit_session() in core/watchlist_v2_session.py at the moment of watchlist insertion.",
      "expires_ts": "STRING (ISO 8601): UTC timestamp when this session will expire. Calculated as added_ts + TTL (default 2 hours / 7200 seconds). After this time, the entry is archived and removed from active watchlist. Source: Computed as added_ts + WATCHLIST_TTL_HOURS in admit_session().",
      "session_id": "STRING: Unique identifier for this tracking session. Current implementation uses UUID v4 format (e.g., 'c1ffb3c1-519e-49df-9328-a8d03aabae7c'). Legacy entries may use '{symbol}-{added_ts}' format. Used to track the session across admission, sampling, and archival. Source: Generated via uuid.uuid4() in admit_session() in core/watchlist_session.py.",
      "sample_count": "INTEGER: Running count of price samples collected during this session. Incremented every ~10 seconds during the sampling loop. At session end (2 hours), typically reaches ~700-720 samples. Source: Incremented by sample_active_sessions() in core/watchlist_v2_session.py.",
      "last_sample_ts": "STRING (ISO 8601): UTC timestamp of the most recent price sample collected for this session. Updated every ~10 seconds during active sampling. Source: Updated by sample_active_sessions() after each successful price fetch.",
      "expired_ts": "STRING (ISO 8601): UTC timestamp of when this session was actually archived (moved from active watchlist to archive). May be slightly after expires_ts due to archiver timing. Source: Set by _archive_entry() in core/watchlist_archiver.py at archival time.",
      "duration_sec": "FLOAT: Total session duration in seconds from admission to archival. Calculated as (expired_ts - added_ts).total_seconds(). Typically ~7200 seconds (2 hours) but may vary slightly based on archiver timing. Source: Computed in _archive_entry() in core/watchlist_archiver.py.",
      "total_samples": "INTEGER: Final count of price samples collected during the entire session. Copied from sample_count at archival time. Typically 700-761 samples for a full 2-hour session at 10-second intervals. Source: Set by _archive_entry() from meta.sample_count.",
      "archive_schema_version": "INTEGER: The schema version of the entry at the time of archival. Copied from schema_version to preserve the original schema even if migrations occur. Used for archive compatibility. Source: Set by _archive_entry() from meta.schema_version."
    },
    "spot_raw": {
      "mid": "FLOAT: Mid-price calculated as (bid + ask) / 2, in quote currency (USD for USDC pairs). This is the theoretical fair price at the moment of snapshot. Source: Computed from Binance spot API bookTicker endpoint bid/ask prices in build_spot_raw_v5().",
      "bid": "FLOAT: Best bid price on the order book, in quote currency. This is the highest price a buyer is willing to pay. Source: Binance spot API bookTicker endpoint 'bidPrice' field.",
      "ask": "FLOAT: Best ask price on the order book, in quote currency. This is the lowest price a seller is willing to accept. Source: Binance spot API bookTicker endpoint 'askPrice' field.",
      "spread_bps": "FLOAT: Bid-ask spread expressed in basis points (1 bps = 0.01%). Formula: 10000 * (ask - bid) / mid. Lower values indicate tighter, more liquid markets. Example: 7.5 bps means the spread is 0.075% of the mid price. Source: Computed in build_spot_raw_v5() from bid/ask prices.",
      "last": "FLOAT: Last traded price on the spot market, in quote currency. May differ from mid price if recent trades moved the market. Source: Binance spot API ticker/24hr endpoint 'lastPrice' field.",
      "range_pct_24h": "FLOAT: 24-hour price range as a percentage of mid price. Formula: 100 * (high24 - low24) / mid. Measures intraday volatility - higher values indicate more price movement. Example: 5.46 means the 24h high-low range was 5.46% of current price. Source: Computed from Binance spot API ticker/24hr high/low prices.",
      "ticker24_chg": "FLOAT: 24-hour price change as a signed percentage. Positive = price increased, negative = price decreased over last 24 hours. Example: -0.149 means price dropped 0.149% in 24h. Source: Binance spot API ticker/24hr endpoint 'priceChangePercent' field.",
      "taker_buy_ratio_5m": "FLOAT (0.0 to 1.0): Proportion of taker (market order) volume that was buy-side over the last 5 minutes. 0.5 = balanced, >0.5 = more aggressive buying, <0.5 = more aggressive selling. Used as a short-term order flow indicator. Source: Computed from Binance spot API aggTrades endpoint, aggregating taker buy vs total volume over 5-minute window.",
      "depth_5bps_quote": "FLOAT: Total order book depth (liquidity) available within +/- 5 basis points of mid price, measured in quote currency (USD). Sum of executable bid and ask quantities within this tight band. Higher values = more liquidity for small trades. Source: Computed from Binance spot API depth endpoint, summing orders within the price band.",
      "depth_10bps_quote": "FLOAT: Total order book depth available within +/- 10 basis points of mid price, in quote currency. Captures slightly wider liquidity than depth_5bps_quote. Source: Computed from Binance spot API depth endpoint.",
      "depth_25bps_quote": "FLOAT: Total order book depth available within +/- 25 basis points of mid price, in quote currency. Represents liquidity available for medium-sized trades. Source: Computed from Binance spot API depth endpoint.",
      "depth_bid_qty_quote": "FLOAT: Total bid-side (buy order) depth within the configured depth window, in quote currency. Used to assess buying pressure and support levels. Source: Computed from Binance spot API depth endpoint bid orders.",
      "depth_ask_qty_quote": "FLOAT: Total ask-side (sell order) depth within the configured depth window, in quote currency. Used to assess selling pressure and resistance levels. Source: Computed from Binance spot API depth endpoint ask orders.",
      "obi_5": "FLOAT (-1.0 to 1.0): Order Book Imbalance calculated from top-of-book bid/ask quantities. Formula: (bid_qty - ask_qty) / (bid_qty + ask_qty). Positive values (0 to +1) indicate more bid quantity than ask (buying pressure). Negative values (-1 to 0) indicate more ask quantity (selling pressure). Zero means balanced. The '_5' suffix is a naming convention. Source: Computed in base_scorer.py from Binance bookTicker endpoint bidQty/askQty fields.",
      "micro_premium_pct": "FLOAT: Microstructure premium/discount as a percentage. Measures the difference between size-weighted mid price and simple mid price. Formula: 100 * (weighted_mid - simple_mid) / simple_mid. Positive = buyers paying premium, negative = discount. Indicates short-term directional pressure. Source: Computed in base_scorer.py using order book size weights.",
      "avg_impact_pct": "FLOAT: Average expected price impact as a percentage for configured trade sizes. Estimates slippage cost for market orders. Higher values indicate thinner liquidity. Source: Computed from order book depth analysis, measuring how much the price would move for standard clip sizes.",
      "spread_eff_raw": "FLOAT: Spread efficiency metric measuring spread cost relative to price volatility. Formula: (spread_bps/10000) / (range_pct_24h/100). Lower values are better - indicates tight spreads relative to the coin's natural price movement. Example: 0.01 means spread is 1% of the 24h range (excellent). 1.0 means spread equals the full range (poor). Source: Computed in base_scorer.py compute_stage1_meta_scores().",
      "liq_eff_raw": "FLOAT: Liquidity efficiency metric combining liquidity score with spread quality. Formula: liq_score / (1.0 + spread_bps). Higher values are better - indicates good liquidity that isn't offset by wide spreads. The liq_score component (0-100) measures order book depth, volume, and market quality. Source: Computed in base_scorer.py compute_stage1_meta_scores().",
      "liq_qv_usd": "FLOAT: Liquidity proxy in quote currency (USD). Represents the 24-hour quote volume, indicating how much USD-denominated trading activity occurred. Higher values indicate more liquid, actively traded markets. Source: Binance spot API ticker/24hr endpoint 'quoteVolume' field."
    },
    "futures_raw": {
      "_SECTION_NOTE": "HISTORICAL NAMING: Fields with '_1h' suffix (open_interest_1h_delta_pct, top_long_short_accounts_1h, top_long_short_positions_1h) actually use 5-minute API windows, not 1-hour. The '_1h' naming was preserved for backward compatibility with existing archives and analysis pipelines. See individual field descriptions for details.",
      "contract": "STRING: The USDT-margined perpetual futures contract symbol associated with this spot pair. Typically follows pattern '{BASE}USDT' for spot pairs like '{BASE}USDC'. Example: spot ALTUSDC uses futures contract ALTUSDT. Source: Mapped from spot symbol in features/futures_sentiment.py using get_futures_contract() lookup.",
      "last_updated_ts": "STRING (ISO 8601 with Z suffix): UTC timestamp of when the futures data block was last refreshed from Binance API. Used to track data freshness. Format: 'YYYY-MM-DDTHH:MM:SS.ssssssZ'. Source: Set to current time when futures data is fetched in features/futures_sentiment.py.",
      "age_sec": "INTEGER: Staleness of the futures data in seconds at the time of snapshot. Calculated as (snapshot_ts - last_updated_ts). Zero means freshly fetched. Higher values indicate cached/stale data. Source: Computed when entry is built, comparing snapshot time to last_updated_ts.",
      "funding_now": "FLOAT (signed): Current/most recent perpetual futures funding rate per 8-hour interval. Positive = longs pay shorts (bullish bias in market). Negative = shorts pay longs (bearish bias). Typical range: -0.001 to +0.001 (-0.1% to +0.1%). Example: 0.00005 = 0.005% funding rate. Source: Binance FAPI /fapi/v1/fundingRate endpoint 'fundingRate' field.",
      "funding_24h_mean": "FLOAT (signed): Mean funding rate over the trailing 24 hours (3 x 8-hour intervals). Smooths out single-interval spikes. Used to detect sustained positioning bias. Persistently positive = strong bullish sentiment, persistently negative = bearish. Source: Computed as average of last 3 funding rate records from Binance FAPI.",
      "open_interest": "FLOAT: Current total open interest in USD (quote currency). Represents the total value of all outstanding perpetual futures contracts. Higher OI = more capital deployed in derivatives. Rapid OI increases often precede volatility. Source: Binance FAPI /fapi/v1/openInterest endpoint, converted to USD using mark price.",
      "open_interest_1h_delta_pct": "FLOAT (signed percentage): Percent change in open interest comparing latest vs previous data point. NOTE: Despite the '_1h' suffix, the actual API call uses period='5m' with limit=30, so this compares the two most recent 5-minute OI snapshots. Positive = OI increasing (new positions opening). Negative = OI decreasing (positions closing). Source: Computed from Binance FAPI /futures/data/openInterestHist endpoint with period='5m'.",
      "basis_now_bps": "FLOAT (signed, basis points): Current futures premium/discount vs index price. Formula: 10000 * (futures_mark_price - index_price) / index_price. Positive = futures trading at premium (bullish). Negative = futures at discount (bearish). Example: -3.29 bps means futures is 0.0329% below index. Source: Computed from Binance FAPI /fapi/v1/premiumIndex markPrice and indexPrice.",
      "top_long_short_accounts_1h": "FLOAT (ratio): Ratio of long accounts to short accounts among top traders. NOTE: Despite the '_1h' suffix, the actual API call uses period='5m', so this reflects the most recent 5-minute window. >1.0 = more top traders are long. <1.0 = more top traders are short. =1.0 = balanced. Example: 2.26 means top traders have 2.26x more long accounts than short. Source: Binance FAPI /futures/data/topLongShortAccountRatio endpoint with period='5m'.",
      "top_long_short_positions_1h": "FLOAT (ratio): Ratio of long positions to short positions (by value/size) among top traders. NOTE: Despite the '_1h' suffix, the actual API call uses period='5m', so this reflects the most recent 5-minute window. >1.0 = top traders net long. <1.0 = top traders net short. Differs from account ratio by weighting position sizes. Source: Binance FAPI /futures/data/topLongShortPositionRatio endpoint with period='5m'."
    },
    "derived": {
      "_SECTION_NOTE": "Computed composite metrics derived from raw market data. These are NOT raw API responses but calculated values. Most are set at admission from scoring factors; spread_pct and spread_bps are updated by the sampler during the 2-hour session.",
      "depth_spread_bps": "FLOAT (basis points): Current bid-ask spread expressed in basis points, computed from order book data at scoring time. Formula: ((ask - bid) / bid) * 10000. Higher values = wider spread = less liquid market. Example: 7.5 bps means spread is 0.075% of bid price. Source: Computed in base_scorer.py from bid_used/ask_used prices.",
      "depth_weighted": "FLOAT (USD): Weighted aggregate order book depth across multiple price bands (5bps, 10bps, 25bps from mid). Formula: (depth_5bps * 0.5) + (depth_10bps * 0.3) + (depth_25bps * 0.2). Each depth_Xbps value is total USD notional (bid+ask) within X basis points of mid. Prioritizes tightest liquidity (50% weight to 5bps band). Higher values indicate deeper, more liquid order books. Source: Computed in base_scorer.py from Binance spot depth API.",
      "depth_imbalance": "FLOAT (ratio, -1 to +1): Order book imbalance ratio. Formula: (bid_notional_5bps - ask_notional_5bps) / (bid_notional_5bps + ask_notional_5bps). Positive = more bids than asks (buying pressure). Negative = more asks than bids (selling pressure). Zero = balanced book. Source: Computed in base_scorer.py from 5bps depth band notional values.",
      "depth_skew": "FLOAT (ratio, -1 to +1): Skew measure of order book depth distribution. Formula: (bid_volume - ask_volume) / (bid_volume + ask_volume). Positive = bid-heavy book. Negative = ask-heavy book. Zero = symmetric book. Often 0.0 when depth data unavailable. Source: Computed in base_scorer.py from aggregated bid/ask volumes.",
      "flow": "FLOAT (0-100 score): Order flow factor score derived from taker buy ratio. Measures aggressive buying vs selling pressure. Formula: Maps taker_buy_ratio [0.3-0.7] to [0-100] via curve or linear scaling. 50 = neutral flow, >50 = buying pressure, <50 = selling pressure. 0 often indicates flow data unavailable. Source: Computed in base_scorer.py from Binance aggTrades taker buy ratio.",
      "liq_global_pct": "FLOAT (0-100 percentile): Global liquidity percentile rank across ALL trading pairs in the universe. Measures this coin's 24h quote volume relative to all other coins. Formula: percentile_rank(this_coin_qv, all_coins_qv) * 100. Higher = more liquid than peers. Example: 31.7 means this coin has more volume than ~32% of all coins. Source: Computed in base_scorer.py using bisect_right on sorted global volume baseline.",
      "liq_self_pct": "FLOAT (0-100 percentile): Self-liquidity percentile rank comparing this coin's current volume to its OWN 30-day rolling baseline. Measures whether current volume is high or low FOR THIS COIN. Formula: percentile_rank(today_qv, symbol_30d_baseline) * 100. Higher = volume is higher than usual. Example: 31.7 means today's volume exceeds ~32% of the coin's recent daily volumes. Source: Computed in base_scorer.py using symbol-specific baseline from baseline_manager.",
      "spread_pct": "FLOAT (percentage): Current bid-ask spread as a percentage. Formula: spread_bps / 100. Example: 0.0756 means 0.0756% spread. NOTE: This field is updated by the sampler every ~10 seconds during the 2-hour session, so the archived value reflects the LAST sample before expiry. Source: Computed in watchlist_sampler.py from live bid/ask prices.",
      "spread_bps": "FLOAT (basis points): Current bid-ask spread in basis points. Formula: ((ask - bid) / mid) * 10000. Example: 7.56 bps = 0.0756% spread. NOTE: This field is updated by the sampler every ~10 seconds during the 2-hour session, so the archived value reflects the LAST sample before expiry. Source: Computed in watchlist_sampler.py from live bid/ask prices."
    },
    "norm": {
      "_SECTION_NOTE": "RESERVED FOR FUTURE USE. Normalization factors for cross-sectional analysis (z-scores, percentile ranks, universe-relative scaling). Currently empty because feature flag WATCHLIST_USE_NORM_FIELDS is disabled. When enabled, would contain: price_scale, volume_scale, z_score_mom, z_score_vol, rank_percentile."
    },
    "scores": {
      "_SECTION_NOTE": "Internal factor scores used for ranking and admission decisions. Each score is a normalized number from 0 to 100, where the mapping from raw market data to score varies by factor type. These are NOT percentages - they are arbitrary scoring units designed to make factors comparable. The 'final' score combines individual factors with configurable weights. Use these scores to understand WHY a coin was selected and to build your own filtering/ranking logic.",
      "final": "FLOAT (0-100): Weighted composite score determining admission eligibility. Formula: sum(factor_score * factor_weight) + meta_contributions + pair_bonus, clamped to [0,100]. Weights loaded from weights.json (default: liq=0.15, mom=0.10, vol=0.10, str=0.05, spread=0.15, taker=0.15, flow=0.10, depth=0.10, microstruct=0.10). INTERPRETATION: >=60 typically passes admission threshold. 80+ = excellent candidate (strong on most factors). 40-60 = marginal (mixed signals). <40 = poor candidate. Source: base_scorer.py _compute_final_score_and_verdict().",
      "mom": "FLOAT (0-100): Momentum score based on 24h price change percentage. Maps price movement to a bidirectional score centered at 50. Formula: 50 + (price_change_pct / 10) * 50, clamped to [0,100]. INTERPRETATION: 50 = no change (0% move). 60 = +2% gain. 70 = +4% gain. 80 = +6% gain. 100 = +10% or more gain. 40 = -2% loss. 30 = -4% loss. 20 = -6% loss. 0 = -10% or more loss. USE CASE: Filter for uptrending coins (mom > 55) or avoid falling knives (mom < 45). Source: base_scorer.py _calculate_momentum_from_chg().",
      "vol": "FLOAT (0-100): Volatility score based on 24h price range as percentage of price. Formula: (range_pct / 6.0) * 100, where range_pct = (high - low) / last * 100. INTERPRETATION: 0-20 = very low volatility (<1.2% range, stablecoin-like). 30-50 = moderate volatility (1.8-3% range, typical altcoin). 60-80 = high volatility (3.6-4.8% range, active trading). 90-100 = extreme volatility (>5.4% range, potential breakout or dump). USE CASE: Target vol=40-70 for momentum trading; avoid vol<20 (dead) or vol>90 (too risky). Source: base_scorer.py _calculate_volatility_factor().",
      "str": "FLOAT (0-100): Strength score measuring where current price sits within 24h range. Formula: 100 * (last - low) / (high - low). INTERPRETATION: 0-20 = price near daily low (weak, potentially oversold). 30-50 = price in lower half of range (bearish bias). 50 = price at midpoint. 60-70 = price in upper half (bullish bias). 80-100 = price near daily high (strong, potentially overbought). USE CASE: Combine with momentum - high str + high mom = strong uptrend; low str + low mom = downtrend. Source: base_scorer.py strength calculation.",
      "liq": "FLOAT (0-100): Liquidity score based on 24h quote volume percentile ranking. Uses blend of global (30%) and self (70%) percentiles. INTERPRETATION: 0-20 = bottom quintile liquidity (thin order books, high slippage risk). 30-50 = below average liquidity. 50-70 = average liquidity for the universe. 70-90 = above average (good execution quality). 90-100 = top decile (major coins, excellent depth). USE CASE: Require liq >= 30 to avoid illiquid traps; prefer liq >= 50 for reliable fills. Source: base_scorer.py _calculate_liq_factor_bps().",
      "spread": "FLOAT (0-100): Spread score where TIGHTER spreads score HIGHER (inverted). Maps bid-ask spread to score via: 100 for <=0.05% spread, 0 for >=2% spread, linear between. INTERPRETATION: 90-100 = excellent spread (<0.15%, major pairs). 70-90 = good spread (0.15-0.5%, liquid altcoins). 50-70 = acceptable spread (0.5-1%, mid-tier). 30-50 = wide spread (1-1.5%, caution). 0-30 = very wide (>1.5%, avoid for momentum). USE CASE: Require spread >= 70 for active trading; accept spread >= 50 for swing trades. Source: base_scorer.py _calculate_spread_score().",
      "taker": "FLOAT (0-100): Taker buy ratio score measuring aggressive buying vs selling. Maps taker_buy_ratio (0-1) to score. INTERPRETATION: 50 = balanced (equal buy/sell taker volume). 60-70 = moderate buying pressure. 70-85 = strong buying pressure (accumulation). 85-100 = extreme buying (potential FOMO). 30-50 = moderate selling. 0-30 = heavy selling (distribution/panic). USE CASE: Prefer taker > 55 for long entries; avoid taker < 45 (sellers dominating). Source: base_scorer.py taker scoring.",
      "spread_eff_score": "FLOAT (0-100): Spread efficiency meta-factor comparing spread cost to volatility. Formula: 100 * (1 - spread_decimal/range_decimal), clamped. INTERPRETATION: 90-100 = spread is <10% of daily range (excellent value). 70-90 = spread is 10-30% of range (good). 50-70 = spread is 30-50% of range (fair). 30-50 = spread is 50-70% of range (poor value). 0-30 = spread exceeds 70% of range (avoid - spread eats profits). USE CASE: High spread_eff means you get more price movement for the spread cost. Source: base_scorer.py _calculate_meta_factors().",
      "liq_eff_score": "FLOAT (0-100): Liquidity efficiency meta-factor rewarding good liquidity with tight spreads. Formula: log-scaled mapping of liq/(1+spread_bps). INTERPRETATION: 80-100 = excellent liquidity AND tight spread (ideal). 60-80 = good balance. 40-60 = acceptable. 20-40 = either low liquidity or wide spread. 0-20 = poor on both dimensions. USE CASE: Prioritize coins with liq_eff >= 60 for best execution quality. Source: base_scorer.py _calculate_meta_factors().",
      "flow": "FLOAT (0-100): Order flow score from taker analysis (same underlying data as taker score, may use different curve). Legacy formula: ((taker_buy_ratio - 0.3) / 0.4) * 100 for ratio in [0.3, 0.7]. INTERPRETATION: Same as taker - 50 = neutral, >50 = buying pressure, <50 = selling. Often equals taker score or is very close. USE CASE: Cross-check with taker; consistent signals are more reliable. Source: base_scorer.py flow factor calculation.",
      "depth": "FLOAT (0-100): Order book depth score based on USD liquidity within tight price bands (5/10/25 bps from mid). Uses weighted depth and percentile ranking. INTERPRETATION: 80-100 = deep books (>$100k within 25bps, institutional grade). 60-80 = good depth ($30-100k). 40-60 = moderate depth ($10-30k). 20-40 = thin books ($3-10k). 0-20 = very thin (<$3k, high slippage). USE CASE: Require depth >= 40 for reliable execution; depth >= 60 preferred for larger positions. Source: base_scorer.py _calculate_depth_factor_from_notional().",
      "microstruct": "FLOAT (0-100): Microstructure composite score combining OBI (order book imbalance) and micro premium indicators. Formula: 0.6*obi_factor + 0.4*micro_factor. INTERPRETATION: 50 = neutral/balanced order book. 60-70 = mild bid-side pressure (supportive). 70-85 = strong bid support (bullish microstructure). 85-100 = extreme imbalance (may reverse). 30-50 = mild ask pressure. 0-30 = heavy ask imbalance (bearish). USE CASE: Prefer microstruct > 50 for longs; be cautious if microstruct < 40. Source: base_scorer.py microstructure scoring.",
      "compression_score": "FLOAT (0-100): Volatility compression meta-factor measuring room for price expansion. Formula: inverse mapping of range_pct/std_pct ratio. INTERPRETATION: 80-100 = low compression (price has room to move, not squeezed). 50-80 = moderate compression. 30-50 = high compression (price may be coiling). 0-30 = extreme compression (breakout or breakdown likely). Default 50 when disabled. USE CASE: Look for compression_score 30-50 combined with other bullish signals for breakout setups. Source: base_scorer.py _calculate_meta_factors()."
    },
    "flags": {
      "_SECTION_NOTE": "Boolean indicators about data quality and scoring behavior. Use these to filter entries with incomplete or degraded data. true = condition met/enabled, false = condition not met/disabled. When building ML models or analysis, consider filtering on spot_data_ok=true and checking fallback flags to ensure data quality.",
      "spot_data_ok": "BOOLEAN: true if spot market data was successfully fetched and contains valid mid price. false indicates API failure or missing data - the entry may have degraded scoring accuracy. USE: Filter entries where spot_data_ok=false for clean datasets. Source: Checked in build_flags_v5() as bool(spot_raw and spot_raw.get('mid')).",
      "futures_data_ok": "BOOLEAN: true if futures data was successfully fetched and contains a valid contract field. false means futures endpoints failed or this coin has no perpetual contract. Note: false is NORMAL for coins without USDT-M futures (check futures_contract_exists). USE: When analyzing futures sentiment, filter for futures_data_ok=true. Source: Checked in build_flags_v5() as bool(futures_raw and futures_raw.get('contract')).",
      "futures_stale": "BOOLEAN: true if futures data age exceeds 2x the configured TTL (default: age > 600 seconds). Indicates cached/outdated futures data that may not reflect current market conditions. false means futures data is fresh. USE: For time-sensitive futures analysis, prefer futures_stale=false. Source: Computed in _check_futures_stale() comparing age_sec to 2*FUTURES_TTL_OI_SEC.",
      "compression_enabled": "BOOLEAN: true if volatility compression scoring was active for this entry. Compression measures range_pct/std_pct ratio to detect price coiling. false means compression_score defaults to 50 (neutral). Controlled by env COMPRESSION_ENABLE. USE: Only use compression_score when compression_enabled=true. Source: From factors.compression_enabled.",
      "mom_fallback": "BOOLEAN: true if momentum calculation fell back to an alternate method (e.g., RSI when 4h klines unavailable, or default value when all methods failed). false means primary momentum calculation succeeded. USE: Entries with mom_fallback=true may have less reliable momentum scores. Source: From factors.mom_fallback set in base_scorer.py.",
      "vol_fallback": "BOOLEAN: true if volatility calculation fell back to an alternate method (e.g., range_pct when ATR unavailable). false means primary volatility method succeeded. USE: Entries with vol_fallback=true may have less precise volatility scoring. Source: From factors.vol_fallback set in base_scorer.py.",
      "spread_fallback": "BOOLEAN: true if spread calculation used fallback data (e.g., cached prices when live book unavailable). false means spread was calculated from fresh order book data. USE: Entries with spread_fallback=true may have stale spread information. Source: From factors.spread_fallback_used set in base_scorer.py.",
      "pair_bonus_applied": "FLOAT (0.0 to ~5.0): The pair bonus points ADDED to the final score. This is NOT a boolean despite being in flags - it's the actual bonus value. Pair bonus rewards factor combinations (e.g., high liq + tight spread). 0.0 = no bonus applied (feature disabled or no qualifying pairs). Typical range: 0.3-1.5 when active. USE: To see raw score before bonus, subtract this from scores.final. Source: From factors.pair_bonus_applied computed in _compute_final_score_and_verdict().",
      "twitter_data_ok": "BOOLEAN: true if Twitter sentiment data was successfully captured for this coin. false means twscrape lookup failed or returned no data - twitter_sentiment_windows may be empty or stale. USE: Filter for twitter_data_ok=true when using sentiment features. Source: Set by cryptobot during sentiment snapshot capture.",
      "futures_contract_exists": "BOOLEAN: true if this coin has a USDT-margined perpetual futures contract on Binance. false means no futures contract exists (legitimate - not all coins have futures). IMPORTANT: Distinguishes expected vs unexpected missing futures data: futures_contract_exists=false + futures_raw=null is NORMAL (no contract). futures_contract_exists=true + futures_raw=null is ERROR (fetch failed). Source: Checked against Binance exchangeInfo in features/futures_sentiment.py.",
      "futures_contract_check_failed": "BOOLEAN: true if the exchangeInfo API call to verify futures contract existence failed. When true, futures_contract_exists defaults to true (conservative assumption). false means contract existence was reliably determined. USE: If futures_contract_check_failed=true, futures_contract_exists may be inaccurate. Source: Set when exchangeInfo request fails in futures_sentiment.py."
    },
    "diag": {
      "_SECTION_NOTE": "Diagnostic metadata for troubleshooting and quality assurance. NOT intended as ML features - these track build process health and validation status. Use these to identify entries with build issues or missing validation.",
      "builder_version": "STRING: Version identifier for the watchlist_builder.py module that created this entry. Currently 'v1.0'. Used to track schema/logic changes over time. If builder logic changes significantly, this version increments. Source: Hardcoded constant in build_diag_v5().",
      "build_duration_ms": "FLOAT: Time in milliseconds to construct this watchlist entry, from start of build_watchlist_entry() to completion. Typical range: 0.1-5.0 ms. High values (>50ms) may indicate API latency or processing issues. USE: Monitor for performance regressions. Source: Computed as (end_time - start_time) * 1000 in build_diag_v5().",
      "admission_validated": "BOOLEAN: true if this entry passed the admission validation gate before being added to the active watchlist. false or missing indicates the entry bypassed validation (should not happen in production). CRITICAL: Entries with admission_validated=false are evicted without archiving when STRICT_ADMISSION_VALIDATION=1. USE: Filter for admission_validated=true in analysis to ensure data quality. Source: Set by admission validator in main_loop.py after gate checks pass.",
      "admission_validation_ts": "STRING (ISO 8601): UTC timestamp of when admission validation completed. Records the exact moment the entry was approved for watchlist insertion. Allows correlation with other system events. Source: Set by admission validator at validation completion time."
    },
    "labels": {
      "_SECTION_NOTE": "RESERVED FOR LABELING PIPELINE. Labels are NOT stored in archived entries - they are computed separately during analysis/ML training. This block is intentionally empty at archival time. Post-hoc labels (outcome_4h, trade_result, etc.) are added by analysis scripts when building datasets."
    },
    "twitter_sentiment_windows": {
      "last_cycle": {
        "posts_total": "INTEGER: Total number of tweets collected and analyzed for this cryptocurrency during the most recent scraping cycle. A 'cycle' is one complete pass through all tracked coins in the scraper's round-robin schedule. The duration of a cycle varies based on API rate limits and queue size (typically 45-90 minutes). Source: Sum of all tweets for this coin in the cycle.",
        "posts_pos": "INTEGER: Count of tweets classified as POSITIVE sentiment using lexicon-based analysis. A tweet is positive if its sentiment score > 0.1 on a scale of -1.0 to +1.0. The lexicon matches terms from categories: positive_general (bullish, moon, hodl, gains, etc.) and pump_hype (pump, breakout, rally, etc.). Source: Computed by aggregation_utils.aggregate_sentiment_metrics().",
        "posts_neu": "INTEGER: Count of tweets classified as NEUTRAL sentiment using lexicon-based analysis. A tweet is neutral if its sentiment score is between -0.1 and +0.1 on a scale of -1.0 to +1.0. These tweets contain no strong positive or negative sentiment signals. Source: Computed by aggregation_utils.aggregate_sentiment_metrics().",
        "posts_neg": "INTEGER: Count of tweets classified as NEGATIVE sentiment using lexicon-based analysis. A tweet is negative if its sentiment score < -0.1 on a scale of -1.0 to +1.0. The lexicon matches terms from categories: negative_general (bearish, crash, dump, etc.), fud_fear (fud, scam, rugpull, etc.), and scam_rug. Source: Computed by aggregation_utils.aggregate_sentiment_metrics().",
        "lexicon_sentiment": {
          "scale": "STRING: Describes the range of the sentiment score. Always '-1 to 1' indicating the score is normalized between -1.0 (extremely negative) and +1.0 (extremely positive).",
          "score": "FLOAT or NULL: Engagement-weighted average sentiment score across all tweets in this cycle. Each tweet's sentiment is weighted by its engagement (likes + 2*replies + 2*retweets), so viral tweets have more influence. Score is computed as: (positive_term_matches - negative_term_matches) / 5.0, clamped to [-1.0, +1.0]. NULL if fewer than 5 tweets in the cycle (bucket_min_posts_for_score threshold). Source: Computed by aggregation_utils.aggregate_sentiment_metrics() using lexicon word matching."
        },
        "category_counts": {
          "positive_general": "INTEGER: Count of positive general sentiment term matches across all tweets. Terms include: bullish, bull, moon, rocket, good, great, buy, hodl, gains, profit, green, ath, etc. Multiple matches in one tweet are counted. Source: Lexicon matching in aggregation_utils.py.",
          "negative_general": "INTEGER: Count of negative general sentiment term matches across all tweets. Terms include: bearish, bear, crash, dump, tank, sell, panic, fear, losses, red, rekt, liquidation, etc. Source: Lexicon matching in aggregation_utils.py.",
          "pump_hype": "INTEGER: Count of pump/hype term matches across all tweets. Terms include: pump, pumping, breakout, rally, surge, fomo, ape, send-it, etc. These indicate extreme bullish excitement. Source: Lexicon matching in aggregation_utils.py.",
          "fud_fear": "INTEGER: Count of FUD (Fear, Uncertainty, Doubt) term matches across all tweets. Terms include: dump, dumping, rug, rugpull, fud, scam, fraud, exit-liquidity, etc. Source: Lexicon matching in aggregation_utils.py.",
          "meme_slang": "INTEGER: Count of crypto meme/slang term matches across all tweets. Terms include: gm, gn, wagmi, ngmi, lfg, degen, ser, fren, copium, hopium, based, chad, etc. These are community culture indicators, not sentiment. Source: Lexicon matching in aggregation_utils.py.",
          "scam_rug": "INTEGER: Count of explicit scam/rug-pull warning term matches. Terms include: scam, rug, rugpull, honeypot, fraud, fake, ponzi. These are severe negative indicators. Source: Lexicon matching in aggregation_utils.py.",
          "emoji_pos": "INTEGER: Count of positive sentiment emojis detected across all tweets. Includes rocket \ud83d\ude80, moon \ud83c\udf19, fire \ud83d\udd25, money \ud83d\udcb0, chart up \ud83d\udcc8, etc. Source: Emoji detection in sentiment scoring.",
          "emoji_neg": "INTEGER: Count of negative sentiment emojis detected across all tweets. Includes skull \ud83d\udc80, chart down \ud83d\udcc9, warning \u26a0\ufe0f, etc. Source: Emoji detection in sentiment scoring."
        },
        "top_terms": {
          "positive_general": "ARRAY of STRING: Up to 3 most frequently matched positive general terms across all tweets in this cycle. Example: ['bullish', 'gains', 'moon']. Empty array if no matches. Source: Counter.most_common(3) on matched terms in aggregation_utils.py.",
          "negative_general": "ARRAY of STRING: Up to 3 most frequently matched negative general terms. Example: ['sell', 'dump', 'bearish']. Source: Counter.most_common(3) on matched terms.",
          "pump_hype": "ARRAY of STRING: Up to 3 most frequently matched pump/hype terms. Example: ['pump', 'breakout']. Source: Counter.most_common(3) on matched terms.",
          "fud_fear": "ARRAY of STRING: Up to 3 most frequently matched FUD/fear terms. Example: ['fud', 'scam']. Source: Counter.most_common(3) on matched terms.",
          "meme_slang": "ARRAY of STRING: Up to 3 most frequently matched meme/slang terms. Example: ['wagmi', 'lfg', 'degen']. Source: Counter.most_common(3) on matched terms.",
          "scam_rug": "ARRAY of STRING: Up to 3 most frequently matched scam/rug terms. Example: ['scam', 'rug']. Source: Counter.most_common(3) on matched terms."
        },
        "platform_engagement": {
          "total_likes": "INTEGER: Sum of like counts (favorite_count) across all tweets in this cycle. Extracted from Twitter API response stored in each tweet. Source: Sum of tweet.like_count or tweet.favorite_count.",
          "total_retweets": "INTEGER: Sum of retweet counts across all tweets in this cycle. Source: Sum of tweet.retweet_count.",
          "total_replies": "INTEGER: Sum of reply counts across all tweets in this cycle. Source: Sum of tweet.reply_count.",
          "total_quotes": "INTEGER: Sum of quote tweet counts across all tweets in this cycle. Source: Sum of tweet.quote_count.",
          "total_bookmarks": "INTEGER: Sum of bookmark counts across all tweets in this cycle. Extracted from raw.bookmarkedCount in the tweet payload. Source: Sum of tweet.raw.bookmarkedCount.",
          "total_impressions": "INTEGER or NULL: Sum of view/impression counts across all tweets. NULL if Twitter did not provide view counts (older API responses or private tweets). Source: Sum of tweet.view_count, NULL if all zeros or unavailable."
        },
        "tag_counts": "OBJECT: Dictionary mapping lowercase hashtags to their occurrence count across all tweets. Limited to top 20 most frequent. Example: {'bitcoin': 15, 'crypto': 12, 'btc': 8}. Source: Extracted from tweet.raw.hashtags, counted via Counter.most_common(20).",
        "cashtag_counts": "OBJECT: Dictionary mapping UPPERCASE cashtags to their occurrence count across all tweets. Limited to top 20 most frequent. Example: {'BTC': 30, 'ETH': 25, 'XRP': 20}. Source: Extracted from tweet.raw.cashtags, counted via Counter.most_common(20).",
        "mention_counts": "OBJECT: Dictionary mapping lowercase Twitter usernames (without @) to their mention count across all tweets. Limited to top 20 most frequent. Example: {'elonmusk': 5, 'binance': 3}. Source: Extracted from tweet.raw.mentionedUsers, counted via Counter.most_common(20).",
        "url_domain_counts": "OBJECT: Dictionary mapping lowercase URL domains to their occurrence count across all tweets. Limited to top 10 most frequent. Example: {'t.me': 25, 'youtube.com': 5}. Source: Domain extracted from tweet.raw.links URLs, counted via Counter.most_common(10).",
        "media_count": "INTEGER: Total count of media items (photos + videos + animated GIFs) across all tweets in this cycle. Source: Sum of lengths of tweet.raw.media.photos, videos, and animated arrays.",
        "author_stats": {
          "distinct_authors_total": "INTEGER: Count of unique Twitter user IDs who authored tweets in this cycle. One author posting 10 tweets counts as 1 distinct author. Source: len(set(tweet.raw.user.id for all tweets)).",
          "distinct_authors_verified": "INTEGER: Count of unique authors with Twitter's legacy verification badge (blue checkmark pre-Twitter Blue). Source: Count of unique user IDs where tweet.raw.user.verified == true.",
          "distinct_authors_blue": "INTEGER: Count of unique authors with Twitter Blue subscription. Source: Count of unique user IDs where tweet.raw.user.blue == true.",
          "followers_count_sum": "INTEGER: Sum of follower counts for all tweet authors. If one author with 10k followers posts 3 tweets, their followers are counted once. Source: Sum of tweet.raw.user.followersCount for unique authors.",
          "followers_count_median": "INTEGER or NULL: Median follower count among unique authors. NULL if no author data available. Source: statistics.median() of follower counts.",
          "followers_count_max": "INTEGER or NULL: Maximum follower count among all tweet authors. Identifies the most influential author. Source: max() of follower counts.",
          "followers_count_mean": "FLOAT or NULL: Average (mean) follower count among unique authors. Source: statistics.mean() of follower counts, rounded to 1 decimal."
        },
        "content_stats": {
          "posts_original": "INTEGER: Count of original tweets (not retweets) in this cycle. Source: Count of tweets where tweet.raw.retweetedTweet is null.",
          "posts_retweets": "INTEGER: Count of retweets in this cycle. Source: Count of tweets where tweet.raw.retweetedTweet is not null.",
          "posts_with_media": "INTEGER: Count of tweets that contain at least one media item (photo, video, or GIF). Source: Count of tweets where len(media.photos) + len(media.videos) + len(media.animated) > 0.",
          "posts_with_links": "INTEGER: Count of tweets that contain at least one URL/link. Source: Count of tweets where tweet.raw.links is non-empty.",
          "posts_with_hashtags": "INTEGER: Count of tweets that contain at least one hashtag. Source: Count of tweets where tweet.raw.hashtags is non-empty.",
          "posts_with_cashtags": "INTEGER: Count of tweets that contain at least one cashtag ($BTC, $ETH, etc.). Source: Count of tweets where tweet.raw.cashtags is non-empty.",
          "posts_with_mentions": "INTEGER: Count of tweets that contain at least one @mention. Source: Count of tweets where tweet.raw.mentionedUsers is non-empty."
        },
        "sentiment_activity": {
          "recent_posts_count": "INTEGER: Count of tweets collected within the last 24 hours relative to the cycle end time. This measures recent activity velocity. Source: Count of tweets where created_at >= (cycle_end - 24 hours).",
          "has_recent_activity": "BOOLEAN: True if recent_posts_count >= 5 (MIN_RECENT_POSTS_FOR_ACTIVITY threshold). Indicates whether the coin has meaningful ongoing Twitter discussion. Source: recent_posts_count >= 5.",
          "is_silent": "BOOLEAN: True if no tweets have been collected for this coin in the last 24 hours (SILENCE_THRESHOLD_HOURS). A silent coin may indicate low market interest or search query issues. Source: hours_since_latest_tweet > 24 or no tweets at all.",
          "latest_tweet_at": "STRING (ISO 8601) or NULL: Timestamp of the most recent tweet collected for this coin, in UTC. Example: '2026-01-14T21:13:02Z'. NULL if no tweets exist. Source: max(tweet.created_at) formatted as ISO 8601.",
          "hours_since_latest_tweet": "FLOAT or NULL: Number of hours elapsed between the latest tweet and the cycle end time. Used to detect stale data. Example: 0.55 means 33 minutes ago. NULL if no tweets. Source: (cycle_end - latest_tweet_at).total_seconds() / 3600.",
          "config": {
            "recent_horizon_hours": "INTEGER: Configuration constant (24). The time window in hours used to count 'recent' posts. Source: RECENT_HORIZON_HOURS constant in aggregation_utils.py.",
            "min_recent_posts_for_activity": "INTEGER: Configuration constant (5). Minimum number of recent posts required for has_recent_activity to be true. Source: MIN_RECENT_POSTS_FOR_ACTIVITY constant.",
            "silence_threshold_hours": "INTEGER: Configuration constant (24). If hours_since_latest_tweet exceeds this, is_silent becomes true. Source: SILENCE_THRESHOLD_HOURS constant."
          }
        },
        "bucket_status": "STRING: Data quality indicator. Values: 'ok' (sufficient data for sentiment analysis), 'no_activity' (zero tweets in cycle), 'insufficient_data' (1-4 tweets, below minimum threshold). Source: Determined by aggregate_sentiment_metrics() based on posts_total.",
        "bucket_has_valid_sentiment": "BOOLEAN: True if posts_total >= bucket_min_posts_for_score (currently 5). When false, sentiment scores may be unreliable due to small sample size. Source: posts_total >= BUCKET_MIN_POSTS_FOR_SCORE.",
        "bucket_min_posts_for_score": "INTEGER: Configuration constant (5). Minimum number of tweets required for sentiment scores to be considered statistically meaningful. Below this threshold, sentiment metrics are set to NULL. Source: BUCKET_MIN_POSTS_FOR_SCORE constant in aggregation_utils.py.",
        "ai_sentiment": {
          "scoring_system": "STRING: Identifies which AI scoring mode is active. Values: 'hybrid' (two-model system with primary + referee), 'single' (one model only), 'none' (AI scoring disabled). Source: get_ai_sentiment_metadata() in hybrid_config.py.",
          "primary_model": "STRING: Name/version of the primary sentiment model. Example: 'run8_final'. This model is a DistilBERT transformer fine-tuned on crypto Twitter data for binary positive/negative classification. Source: PRIMARY_MODEL_NAME from hybrid_config.py environment variable SENTIMENT_MODEL_PRIMARY.",
          "referee_model": "STRING or NULL: Name/version of the referee model used in hybrid mode. Example: 'run7_final'. This model has well-calibrated confidence scores and is used to decide when to override the primary or classify as neutral. NULL in single-model mode. Source: REFEREE_MODEL_NAME from hybrid_config.py environment variable SENTIMENT_MODEL_REFEREE.",
          "posts_scored": "INTEGER: Number of tweets that were successfully scored by the AI model. May be less than posts_total if some tweets failed processing or were too short. Source: Count of tweets with valid AI sentiment output.",
          "posts_pos": "INTEGER: Count of tweets classified as POSITIVE by the AI model (label_3class = +1). In hybrid mode, this uses the final hybrid decision, not raw model output. Source: Count of tweets where label_3class == 1.",
          "posts_neu": "INTEGER: Count of tweets classified as NEUTRAL by the AI model (label_3class = 0). In hybrid mode, tweets are neutral when the referee's confidence is in the uncertain band (0.40-0.60). Source: Count of tweets where label_3class == 0.",
          "posts_neg": "INTEGER: Count of tweets classified as NEGATIVE by the AI model (label_3class = -1). Source: Count of tweets where label_3class == -1.",
          "prob_mean": "FLOAT or NULL: Mean probability score from the primary model across all scored tweets. Range: 0.0 to 1.0, where >0.5 indicates positive sentiment prediction. Source: statistics.mean(tweet.sentiment.prob for all scored tweets).",
          "prob_std": "FLOAT or NULL: Standard deviation of probability scores across all scored tweets. High std indicates mixed/polarized sentiment. Source: statistics.stdev(probabilities). NULL if <2 tweets scored.",
          "prob_min": "FLOAT or NULL: Minimum probability score among all scored tweets. Indicates the most confidently negative prediction. Source: min(probabilities).",
          "prob_max": "FLOAT or NULL: Maximum probability score among all scored tweets. Indicates the most confidently positive prediction. Source: max(probabilities).",
          "label_3class_mean": "FLOAT or NULL: Mean of the 3-class labels (-1, 0, +1) across all scored tweets. Range: -1.0 to +1.0. Positive values indicate overall positive sentiment, negative values indicate overall negative. Example: 0.67 means sentiment skews positive. Source: statistics.mean(label_3class values)."
        },
        "hybrid_decision_stats": {
          "posts_scored": "INTEGER: Number of tweets scored by the hybrid two-model system. Should match ai_sentiment.posts_scored when hybrid mode is active. Source: Count of tweets with sentiment_hybrid field.",
          "posts_pos": "INTEGER: Count of tweets with final hybrid decision of POSITIVE (hybrid_label_3class = 'pos'). Source: Count where hybrid_label_3class == 'pos'.",
          "posts_neu": "INTEGER: Count of tweets with final hybrid decision of NEUTRAL (hybrid_label_3class = 'neu'). Tweets are classified neutral when the referee model's confidence is in the uncertain band (0.40-0.60). Source: Count where hybrid_label_3class == 'neu'.",
          "posts_neg": "INTEGER: Count of tweets with final hybrid decision of NEGATIVE (hybrid_label_3class = 'neg'). Source: Count where hybrid_label_3class == 'neg'.",
          "mean_score": "FLOAT: Mean of hybrid_score_3class values across all tweets. Each tweet gets -1.0 (negative), 0.0 (neutral), or +1.0 (positive). Range: -1.0 to +1.0. Source: statistics.mean(hybrid_score_3class values).",
          "pos_ratio": "FLOAT: Proportion of tweets classified as positive. Range: 0.0 to 1.0. Example: 0.78 means 78% of tweets are positive. Source: posts_pos / posts_scored.",
          "neg_ratio": "FLOAT: Proportion of tweets classified as negative. Range: 0.0 to 1.0. Source: posts_neg / posts_scored.",
          "neu_ratio": "FLOAT: Proportion of tweets classified as neutral. Range: 0.0 to 1.0. Source: posts_neu / posts_scored.",
          "decision_sources": {
            "single_model": "INTEGER: Count of tweets decided using single-model mode (only used when hybrid is disabled). Should be 0 in hybrid mode. Source: Count where decision_source == 'single_model'.",
            "primary_default": "INTEGER: Count of tweets where the primary model's prediction was used because the referee did not override and confidence was outside the neutral band. This is the normal/default decision path. Source: Count where decision_source == 'primary_default'.",
            "referee_override": "INTEGER: Count of tweets where the referee model OVERRODE the primary model's prediction. This happens when referee_conf >= 0.90 AND referee disagrees with primary. Indicates high-confidence corrections. Source: Count where decision_source == 'referee_override'.",
            "referee_neutral_band": "INTEGER: Count of tweets classified as NEUTRAL because the referee model's confidence was in the uncertain band (0.40 <= referee_conf <= 0.60). These are genuinely ambiguous tweets. Source: Count where decision_source == 'referee_neutral_band'."
          },
          "primary_conf_mean": "FLOAT or NULL: Mean confidence score from the primary model across all scored tweets. Range: 0.0 to 1.0. Higher values indicate more confident predictions. Source: statistics.mean(primary_conf values).",
          "referee_conf_mean": "FLOAT or NULL: Mean confidence score from the referee model across all scored tweets. Range: 0.0 to 1.0. This model is calibrated to have meaningful confidence values for decision-making. Source: statistics.mean(referee_conf values)."
        }
      },
      "last_2_cycles": {
        "window_cycles": "INTEGER: Always 2. Indicates this is an aggregation of 2 consecutive scraping cycles. Source: Hardcoded in aggregation_two_cycles.py.",
        "from_cycle_id": "INTEGER: The cycle ID of the earlier (N-1) cycle included in this window. Example: 1068. Source: prev_bucket.cycle_id from aggregation_two_cycles.py.",
        "to_cycle_id": "INTEGER: The cycle ID of the later (N) cycle included in this window. Example: 1069. Source: curr_bucket.cycle_id from aggregation_two_cycles.py.",
        "cycle_start_utc": "STRING (ISO 8601): Start timestamp of the earlier cycle (N-1). This is when the first coin in cycle N-1 began collection. Source: prev_bucket.cycle_start_utc.",
        "cycle_end_utc": "STRING (ISO 8601): End timestamp of the later cycle (N). This is when the last coin in cycle N completed collection. Source: curr_bucket.cycle_end_utc.",
        "posts_total": "INTEGER: SUM of posts_total from both cycles. Total tweets analyzed across the 2-cycle window. Source: prev.posts_total + curr.posts_total.",
        "posts_pos": "INTEGER: SUM of posts_pos from both cycles. Source: prev.posts_pos + curr.posts_pos.",
        "posts_neu": "INTEGER: SUM of posts_neu from both cycles. Source: prev.posts_neu + curr.posts_neu.",
        "posts_neg": "INTEGER: SUM of posts_neg from both cycles. Source: prev.posts_neg + curr.posts_neg.",
        "lexicon_sentiment": {
          "scale": "STRING: Always '-1 to 1'. Source: Copied from current cycle.",
          "score": "FLOAT or NULL: AVERAGE of the two cycles' lexicon sentiment scores. NULL if both cycles have NULL scores. Source: (prev.score + curr.score) / 2, with NULL handling."
        },
        "category_counts": {
          "positive_general": "INTEGER: SUM of positive_general counts from both cycles. Source: merge_dict_counts().",
          "negative_general": "INTEGER: SUM of negative_general counts from both cycles.",
          "pump_hype": "INTEGER: SUM of pump_hype counts from both cycles.",
          "fud_fear": "INTEGER: SUM of fud_fear counts from both cycles.",
          "meme_slang": "INTEGER: SUM of meme_slang counts from both cycles.",
          "scam_rug": "INTEGER: SUM of scam_rug counts from both cycles.",
          "emoji_pos": "INTEGER: SUM of emoji_pos counts from both cycles.",
          "emoji_neg": "INTEGER: SUM of emoji_neg counts from both cycles."
        },
        "top_terms": {
          "negative_general": "ARRAY of STRING: MERGED and DEDUPLICATED list of top terms from both cycles. Preserves order from cycle N-1 then N. Source: merge_top_terms() in aggregation_two_cycles.py.",
          "positive_general": "ARRAY of STRING: MERGED and DEDUPLICATED list from both cycles.",
          "fud_fear": "ARRAY of STRING: MERGED and DEDUPLICATED list from both cycles.",
          "pump_hype": "ARRAY of STRING: MERGED and DEDUPLICATED list from both cycles.",
          "scam_rug": "ARRAY of STRING: MERGED and DEDUPLICATED list from both cycles.",
          "meme_slang": "ARRAY of STRING: MERGED and DEDUPLICATED list from both cycles."
        },
        "platform_engagement": {
          "total_likes": "INTEGER: SUM of likes from both cycles. Source: prev.total_likes + curr.total_likes.",
          "total_retweets": "INTEGER: SUM of retweets from both cycles.",
          "total_replies": "INTEGER: SUM of replies from both cycles.",
          "total_views": "INTEGER: SUM of views/impressions from both cycles. Note: This field is named 'total_views' in aggregated windows (differs from 'total_impressions' in last_cycle).",
          "avg_likes": "FLOAT or NULL: AVERAGE of avg_likes from both cycles. NULL if no data. Note: These avg_* fields only appear in aggregated windows.",
          "avg_retweets": "FLOAT or NULL: AVERAGE of avg_retweets from both cycles.",
          "avg_replies": "FLOAT or NULL: AVERAGE of avg_replies from both cycles.",
          "avg_views": "FLOAT or NULL: AVERAGE of avg_views from both cycles."
        },
        "tag_counts": "OBJECT: MERGED dictionary of hashtag counts from both cycles. Counts are summed for common hashtags. Source: merge_dict_counts(prev.tag_counts, curr.tag_counts).",
        "cashtag_counts": "OBJECT: MERGED dictionary of cashtag counts from both cycles. Counts are summed. Source: merge_dict_counts().",
        "mention_counts": "OBJECT: MERGED dictionary of mention counts from both cycles. Counts are summed. Source: merge_dict_counts().",
        "url_domain_counts": "OBJECT: MERGED dictionary of URL domain counts from both cycles. Counts are summed. Source: merge_dict_counts().",
        "media_count": "INTEGER: SUM of media_count from both cycles. Source: prev.media_count + curr.media_count.",
        "author_stats": {
          "followers_count_sum": "INTEGER: SUM of followers_count_sum from both cycles. Note: May double-count if same author posted in both cycles. Source: prev.followers_count_sum + curr.followers_count_sum.",
          "followers_count_median": "FLOAT or NULL: AVERAGE of the two cycles' median values. This is an approximation since true median would require raw data. Source: (prev.median + curr.median) / 2.",
          "followers_count_mean": "FLOAT or NULL: AVERAGE of the two cycles' mean values. Source: (prev.mean + curr.mean) / 2.",
          "followers_count_max": "INTEGER or NULL: MAX of the two cycles' max values. Finds the highest-follower author across both cycles. Source: max(prev.max, curr.max)."
        },
        "content_stats": {
          "posts_original": "INTEGER: SUM from both cycles. Source: prev.posts_original + curr.posts_original.",
          "posts_retweets": "INTEGER: SUM from both cycles.",
          "posts_with_media": "INTEGER: SUM from both cycles.",
          "posts_with_links": "INTEGER: SUM from both cycles.",
          "posts_with_hashtags": "INTEGER: SUM from both cycles.",
          "posts_with_cashtags": "INTEGER: SUM from both cycles.",
          "posts_with_mentions": "INTEGER: SUM from both cycles."
        },
        "sentiment_activity": {
          "latest_tweet_at": "STRING (ISO 8601): The latest_tweet_at from the MORE RECENT cycle (N). Source: curr.latest_tweet_at.",
          "recent_posts_count": "INTEGER: MAX of recent_posts_count from both cycles. The current cycle's value is most accurate for recency. Source: max(prev.recent_posts_count, curr.recent_posts_count).",
          "has_recent_activity": "BOOLEAN: TRUE if EITHER cycle has recent activity (OR logic). Source: prev.has_recent_activity OR curr.has_recent_activity."
        },
        "bucket_status": "STRING: 'ok' if posts_total > 0, otherwise 'silent'. Simplified status for aggregated windows. Source: Computed in aggregation_two_cycles.py.",
        "bucket_has_valid_sentiment": "BOOLEAN: TRUE if EITHER cycle has valid sentiment (OR logic). At least one cycle met the minimum post threshold. Source: prev.bucket_has_valid_sentiment OR curr.bucket_has_valid_sentiment.",
        "bucket_min_posts_for_score": "INTEGER: Copied from current cycle. Configuration constant (5). Source: curr.bucket_min_posts_for_score.",
        "ai_sentiment": {
          "scoring_system": "STRING: Copied from current cycle. Should be same for both cycles. Source: curr.scoring_system.",
          "primary_model": "STRING: Copied from current cycle. Source: curr.primary_model.",
          "referee_model": "STRING or NULL: Copied from current cycle. Source: curr.referee_model.",
          "posts_scored": "INTEGER: SUM from both cycles. Source: prev.posts_scored + curr.posts_scored.",
          "posts_pos": "INTEGER: SUM from both cycles. Source: prev.posts_pos + curr.posts_pos.",
          "posts_neu": "INTEGER: SUM from both cycles.",
          "posts_neg": "INTEGER: SUM from both cycles.",
          "prob_mean": "FLOAT or NULL: AVERAGE of both cycles' prob_mean. Source: (prev.prob_mean + curr.prob_mean) / 2.",
          "label_3class_mean": "FLOAT or NULL: AVERAGE of both cycles' label_3class_mean. Source: (prev + curr) / 2.",
          "prob_min": "FLOAT or NULL: MIN of both cycles' prob_min. Finds the most negative prediction across both. Source: min(prev.prob_min, curr.prob_min).",
          "prob_max": "FLOAT or NULL: MAX of both cycles' prob_max. Finds the most positive prediction across both. Source: max(prev.prob_max, curr.prob_max)."
        },
        "hybrid_decision_stats": {
          "posts_scored": "INTEGER: SUM from both cycles. Source: prev.posts_scored + curr.posts_scored.",
          "posts_pos": "INTEGER: SUM from both cycles.",
          "posts_neu": "INTEGER: SUM from both cycles.",
          "posts_neg": "INTEGER: SUM from both cycles.",
          "mean_score": "FLOAT: AVERAGE of both cycles' mean_score. Source: (prev.mean_score + curr.mean_score) / 2.",
          "primary_conf_mean": "FLOAT or NULL: AVERAGE of both cycles' primary_conf_mean.",
          "referee_conf_mean": "FLOAT or NULL: AVERAGE of both cycles' referee_conf_mean.",
          "pos_ratio": "FLOAT: RECALCULATED from summed counts: total_pos / total_scored. More accurate than averaging ratios. Source: (prev.posts_pos + curr.posts_pos) / (prev.posts_scored + curr.posts_scored).",
          "neg_ratio": "FLOAT: RECALCULATED from summed counts: total_neg / total_scored.",
          "neu_ratio": "FLOAT: RECALCULATED from summed counts: total_neu / total_scored.",
          "decision_sources": {
            "single_model": "INTEGER: SUM from both cycles. Source: merge_dict_counts().",
            "primary_default": "INTEGER: SUM from both cycles.",
            "referee_override": "INTEGER: SUM from both cycles.",
            "referee_neutral_band": "INTEGER: SUM from both cycles."
          }
        }
      }
    },
    "twitter_sentiment_meta": {
      "source": "POPULATED_BY_CRYPTOBOT: Identifies the data source. Value 'twscrape_snapshot' indicates this data came from the twscrape sentiment system. Set by cryptobot when capturing the snapshot.",
      "captured_at_utc": "POPULATED_BY_CRYPTOBOT: ISO 8601 timestamp with timezone of when cryptobot queried and captured this sentiment data. Example: '2026-01-14T22:08:03.556528+00:00'. Set by cryptobot.",
      "bucket_meta": {
        "platform": "STRING: Always 'twitter'. Identifies the social media platform. Source: PLATFORM constant in cycle_buckets.py.",
        "coin": "STRING: The full trading pair symbol. Example: 'ALTUSDC'. This is the coin identifier used by twscrape for search queries. Source: coin parameter passed to build_cycle_bucket().",
        "asset_unified_id": "STRING: Unified asset identifier, typically same as coin. Example: 'ALTUSDC'. Used for cross-platform data alignment. Source: parse_asset_from_coin() in aggregation_utils.py.",
        "date": "STRING (YYYY-MM-DD): The date of the cycle, derived from cycle_end_utc. Example: '2026-01-14'. Source: cycle_end_dt.strftime('%Y-%m-%d').",
        "bucket_span": "STRING: Always 'cycle'. Indicates this bucket represents one complete scraping cycle (as opposed to 'hourly' or 'daily'). Source: BUCKET_SPAN constant in cycle_buckets.py.",
        "cycle_id": "INTEGER: Unique monotonically increasing identifier for this scraping cycle. Each full pass through all tracked coins increments the cycle_id by 1. Example: 1069. Source: Cycle state tracker in the daemon.",
        "cycle_start_utc": "STRING (ISO 8601): UTC timestamp when this cycle started (first coin began collection). Example: '2026-01-14T20:56:39Z'. Source: From daemon cycle state, or computed from earliest tweet timestamp as fallback.",
        "cycle_end_utc": "STRING (ISO 8601): UTC timestamp when this cycle ended (last coin completed collection). Example: '2026-01-14T21:45:52Z'. Source: From daemon cycle state, or computed from latest tweet timestamp as fallback.",
        "created_at_utc": "STRING (ISO 8601): UTC timestamp when this bucket was created/written to disk. Example: '2026-01-14T21:58:02Z'. Source: datetime.now(timezone.utc) at bucket creation time.",
        "scraper_version": "STRING: Version of the twscrape sentiment scraper. Example: '0.3.0'. Source: SCRAPER_VERSION constant in cycle_buckets.py.",
        "sentiment_model_version": "STRING: Version identifier for the sentiment model system. Example: 'v1.0'. Source: SENTIMENT_MODEL_VERSION constant.",
        "lexicon_version": "STRING: Version of the sentiment lexicon/vocabulary. Example: '2025-11-23_v3'. Indicates when the word lists were last updated. Source: LEXICON_VERSION constant.",
        "platform_cycle_id": "INTEGER: Same as cycle_id. Included for schema compatibility. Source: cycle_id parameter.",
        "is_silent": "BOOLEAN: False for normal buckets with tweets. True for 'silent' buckets created when a coin had zero tweets in a cycle. Source: Determined by build_cycle_bucket() vs build_silent_bucket()."
      },
      "key_used": "POPULATED_BY_CRYPTOBOT: The lookup key cryptobot used to find this coin's sentiment data. Example: 'ALT'. This may differ from the full coin symbol (ALTUSDC) as cryptobot maps trading pairs to search keys. Set by cryptobot."
    },
    "spot_prices": [
      {
        "_SECTION_NOTE": "TIME-SERIES PRICE DATA: Array of ~700 price samples collected every ~10 seconds during the 2-hour tracking session. Use this for backtesting entry/exit timing, calculating MAE/MFE, or analyzing price action patterns. First sample is near session admission (added_ts), last sample is near expiry (expired_ts). The array is appended by watchlist_sampler.py on each sampling tick.",
        "ts": "STRING (ISO 8601): Timestamp of this price sample. ARRAY contains ~700 samples at ~10-second intervals during the 2-hour tracking window.",
        "mid": "FLOAT: Mid price = (bid + ask) / 2",
        "bid": "FLOAT: Best bid price",
        "ask": "FLOAT: Best ask price",
        "spread_bps": "FLOAT: Spread in basis points = (ask - bid) / mid * 10000"
      }
    ]
  }
]